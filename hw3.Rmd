---
title: "U20 Math 559 Bayesian Statistics Homework 3"
date: 'Due: 4/25/2021'
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

*Instruction*: Please type or write your answers clearly and show your work. You are encouraged to use the Rmarkdown version of this assignment as a template to submit your work. Unless stated otherwise, all programming references in the assignment will be in `R`, and the predefined `R` functions used for the problems can all be found on our Canvas site under `DBDA2Eprograms.zip`, unless specified otherwise. For this assignment, problems roughly covers content from lectures 7-9.


**Problem 1 (Modified from Exercise 10.2)** 
Consider the following simulated data

```{r, eval=FALSE}
------------------------
N <- 9
z=6
y = c( rep(0,N-z) , rep(1,z) )
dataList = list(
  y = y ,
  N = N 
)
------------------------
```

and the corresponding Bayesian hierarchical model

```{r,eval=FALSE}
------------------------
modelString = "
model {
  for ( i in 1:N ) {
    y[i] ~ dbern( theta )
  }
  theta ~ dbeta( omega[m]*(kappa-2)+1 , (1-omega[m])*(kappa-2)+1 ) 
  omega[1] <- .25
  omega[2] <- .75
  kappa <- 12
  m ~ dcat( mPriorProb[] )
  mPriorProb[1] <- .5
  mPriorProb[2] <- .5
}
" 
------------------------
```

a) Describe the structure of the hierarchical model and what each argument means. Outline which argument represents the likelihood function, and what is the relationship of each of the parameters in relation to how the prior is defined. Please note, the argument `dcat` represents the categorical distribution which takes a vector of probabilities for each category. JAGS does not allow the vector constants to be defined inside the argument, like this: `m ~ dcat(c(.5,.5))`, therefore it is defined as a placeholder `mPriorProb` first.

likelihood function is found in the line: 
```{r, eval=FALSE}
y[i] ~ dbern( theta ) 
```
beta distribution prior is defined here: 
```{r, eval=FALSE}
theta ~ dbeta( omega[m]*(kappa-2)+1 , (1-omega[m])*(kappa-2)+1 ) 
```
The likelihood function and Beta prior distribution are only defined once for coding convenience even though they they participate in both models. The value of omega[m] in the beta distribution depends on the model index, m. The next lines in the model specification assign values for omega[1] and omega[2].


b) Use the script `Jags-Ydich-Xnom1subj-MbernBetaModelComp.R` to produce the prior and the posterior distribution of the problem above. Explain how you generated the MCMC sample from the prior. Include the graphical output in your answer, which should resemble Figure 1 below (or Figure 10.4 from the book). Note, if you got an error related to `X11 display`, try commenting out the line "`openGraph(width=7,height=5)`". 

```{r}
# Jags-Ydich-Xnom1subj-MbernBetaModelComp.R
# Accompanies the book:
#   Kruschke, J. K. (2014). Doing Bayesian Data Analysis: 
#   A Tutorial with R, JAGS, and Stan. 2nd Edition. Academic Press / Elsevier.
#graphics.off()
rm(list=ls(all=TRUE))
source("DBDA2E-utilities.R")
require(rjags)
fileNameRoot="Jags-Ydich-Xnom1subj-MbernBetaModelComp-" # for output filenames

#------------------------------------------------------------------------------
# THE DATA.

N=9
z=6
y = c( rep(0,N-z) , rep(1,z) )
dataList = list(
  #y = y ,
  N = N 
)

#------------------------------------------------------------------------------
# THE MODEL.

modelString = "
model {
  for ( i in 1:N ) {
    y[i] ~ dbern( theta )
  }
  theta ~ dbeta( omega[m]*(kappa-2)+1 , (1-omega[m])*(kappa-2)+1 ) 
  omega[1] <- .25
  omega[2] <- .75
  kappa <- 12
  m ~ dcat( mPriorProb[] )
  mPriorProb[1] <- .5
  mPriorProb[2] <- .5
}
" # close quote for modelString
writeLines( modelString , con="TEMPmodel.txt" )

#------------------------------------------------------------------------------
# INTIALIZE THE CHAINS.

# Specific initialization is not necessary in this case, 
# but here is a lazy version if wanted:
# initsList = list( theta=0.5 , m=1 ) 

#------------------------------------------------------------------------------
# RUN THE CHAINS.

parameters = c("theta","m") 
adaptSteps = 1000             # Number of steps to "tune" the samplers.
burnInSteps = 1000           # Number of steps to "burn-in" the samplers.
nChains = 4                   # Number of chains to run.
numSavedSteps=50000          # Total number of steps in chains to save.
thinSteps=1                   # Number of steps to "thin" (1=keep every step).
nPerChain = ceiling( ( numSavedSteps * thinSteps ) / nChains ) # Steps per chain.
# Create, initialize, and adapt the model:
jagsModel = jags.model( "TEMPmodel.txt" , data=dataList , # inits=initsList , 
                        n.chains=nChains , n.adapt=adaptSteps )
# Burn-in:
cat( "Burning in the MCMC chain...\n" )
update( jagsModel , n.iter=burnInSteps )
# The saved MCMC chain:
cat( "Sampling final MCMC chain...\n" )
codaSamples = coda.samples( jagsModel , variable.names=parameters , 
                            n.iter=nPerChain , thin=thinSteps )
# resulting codaSamples object has these indices: 
#   codaSamples[[ chainIdx ]][ stepIdx , paramIdx ]

save( codaSamples , file=paste0(fileNameRoot,"Mcmc.Rdata") )


#------------------------------------------------------------------------------
# EXAMINE THE RESULTS.

# Convert coda-object codaSamples to matrix object for easier handling.
mcmcMat = as.matrix( codaSamples , chains=TRUE )
m = mcmcMat[,"m"]
theta = mcmcMat[,"theta"]

# Compute the proportion of m at each index value:
pM1 = sum( m == 1 ) / length( m )
pM2 = 1 - pM1

# Extract theta values for each model index:
thetaM1 = theta[ m == 1 ]
thetaM2 = theta[ m == 2 ]

# Plot histograms of sampled theta values for each model,
# with pM displayed.
#openGraph(width=7,height=5)
par( mar=0.5+c(3,1,2,1) , mgp=c(2.0,0.7,0) )
layout( matrix(c(1,1,2,3),nrow=2,byrow=FALSE) , widths=c(1,2) )
plotPost( m , breaks=seq(0.9,2.1,0.2) , cenTend="mean" , xlab="m" , main="Model Index" )
plotPost( thetaM1 , 
          main=bquote( theta*" when m=1" * " ; p(m=1|D)" == .(signif(pM1,3)) ) , 
          cex.main=1.75 , xlab=bquote(theta) , xlim=c(0,1) )
plotPost( thetaM2 , 
          main=bquote( theta*" when m=2" * " ; p(m=2|D)" == .(signif(pM2,3)) ) , 
          cex.main=1.75 , xlab=bquote(theta) , xlim=c(0,1) )
saveGraph( file=paste0(fileNameRoot,"Post") , type="eps" )


```


```{r}
# Jags-Ydich-Xnom1subj-MbernBetaModelComp.R
# Accompanies the book:
#   Kruschke, J. K. (2014). Doing Bayesian Data Analysis: 
#   A Tutorial with R, JAGS, and Stan. 2nd Edition. Academic Press / Elsevier.
graphics.off()
rm(list=ls(all=TRUE))
source("DBDA2E-utilities.R")
require(rjags)
fileNameRoot="Jags-Ydich-Xnom1subj-MbernBetaModelComp-" # for output filenames

#------------------------------------------------------------------------------
# THE DATA.

N=9
z=6
y = c( rep(0,N-z) , rep(1,z) )
dataList = list(
  y = y ,
  N = N 
)

#------------------------------------------------------------------------------
# THE MODEL.

modelString = "
model {
  for ( i in 1:N ) {
    y[i] ~ dbern( theta )
  }
  theta ~ dbeta( omega[m]*(kappa-2)+1 , (1-omega[m])*(kappa-2)+1 ) 
  omega[1] <- .25
  omega[2] <- .75
  kappa <- 12
  m ~ dcat( mPriorProb[] )
  mPriorProb[1] <- .5
  mPriorProb[2] <- .5
}
" # close quote for modelString
writeLines( modelString , con="TEMPmodel.txt" )

#------------------------------------------------------------------------------
# INTIALIZE THE CHAINS.

# Specific initialization is not necessary in this case, 
# but here is a lazy version if wanted:
# initsList = list( theta=0.5 , m=1 ) 

#------------------------------------------------------------------------------
# RUN THE CHAINS.

parameters = c("theta","m") 
adaptSteps = 1000             # Number of steps to "tune" the samplers.
burnInSteps = 1000           # Number of steps to "burn-in" the samplers.
nChains = 4                   # Number of chains to run.
numSavedSteps=50000          # Total number of steps in chains to save.
thinSteps=1                   # Number of steps to "thin" (1=keep every step).
nPerChain = ceiling( ( numSavedSteps * thinSteps ) / nChains ) # Steps per chain.
# Create, initialize, and adapt the model:
jagsModel = jags.model( "TEMPmodel.txt" , data=dataList , # inits=initsList , 
                        n.chains=nChains , n.adapt=adaptSteps )
# Burn-in:
cat( "Burning in the MCMC chain...\n" )
update( jagsModel , n.iter=burnInSteps )
# The saved MCMC chain:
cat( "Sampling final MCMC chain...\n" )
codaSamples = coda.samples( jagsModel , variable.names=parameters , 
                            n.iter=nPerChain , thin=thinSteps )
# resulting codaSamples object has these indices: 
#   codaSamples[[ chainIdx ]][ stepIdx , paramIdx ]

save( codaSamples , file=paste0(fileNameRoot,"Mcmc.Rdata") )


#------------------------------------------------------------------------------
# EXAMINE THE RESULTS.

# Convert coda-object codaSamples to matrix object for easier handling.
mcmcMat = as.matrix( codaSamples , chains=TRUE )
m = mcmcMat[,"m"]
theta = mcmcMat[,"theta"]

# Compute the proportion of m at each index value:
pM1 = sum( m == 1 ) / length( m )
pM2 = 1 - pM1

# Extract theta values for each model index:
thetaM1 = theta[ m == 1 ]
thetaM2 = theta[ m == 2 ]

# Plot histograms of sampled theta values for each model,
# with pM displayed.
#openGraph(width=7,height=5)
par( mar=0.5+c(3,1,2,1) , mgp=c(2.0,0.7,0) )
layout( matrix(c(1,1,2,3),nrow=2,byrow=FALSE) , widths=c(1,2) )
plotPost( m , breaks=seq(0.9,2.1,0.2) , cenTend="mean" , xlab="m" , main="Model Index" )
plotPost( thetaM1 , 
          main=bquote( theta*" when m=1" * " ; p(m=1|D)" == .(signif(pM1,3)) ) , 
          cex.main=1.75 , xlab=bquote(theta) , xlim=c(0,1) )
plotPost( thetaM2 , 
          main=bquote( theta*" when m=2" * " ; p(m=2|D)" == .(signif(pM2,3)) ) , 
          cex.main=1.75 , xlab=bquote(theta) , xlim=c(0,1) )
saveGraph( file=paste0(fileNameRoot,"Post") , type="eps" )



```


**Problem 2** (Exercise 11.1)


We have a six-sided die, and we want to know whether the probability that the six-dotted face comes up is fair. Thus, we are considering two possible outcomes: six-dots or not six-dots. If the die is fair, the probability of the six-dotted face is 1/6.

a) Suppose we roll the die N = 45 times, intending to stop at that number of rolls. Suppose we get 3 six-dot rolls. What is the two-tailed p value? Hint: Use Equation 11.5 (p. 303) of the book. Try the following. Explain carefully what each line of the script does. Why does it consider the low tail and not the high tail? Explain the meaning of the final result.


```{r,eval=FALSE}
#number of rolls
N = 45 ; 
#number of six-dot rolls
z = 3 ; 
#p(y=1)
theta = 1/6
#low tail bc observed probability (3/45) is less than expected (1/6)
lowTailZ = 0:z
#Cumulative prob of low tail
lowTailP = sum( choose(N,lowTailZ) * theta^lowTailZ * (1-theta)^(N-lowTailZ) )

#two-tail probability
TwoTailP = 2*lowTailP
TwoTailP
```
the two-tailed p value = 0.089.
0.089 > .05, so fail to reject the null hypothesis theta=1/6


b) Suppose that instead of stopping at fixed N, we stop when we get 3 six-dot outcomes. It takes 45 rolls. (Notice this is the same result as the previous part.) What is the two-tailed p value? Hint: Use Equation 11.6 (p. 306). Try the following. Explain carefully what that code does and what its result means. Compare that with part a), what is the key difference?

```{r,eval=FALSE}
#sum( (lowTailZ/N) * choose(N,lowTailZ) * thetaˆlowTailZ * (1-theta)^(N-lowTailZ) )

## tail is computed by 1-p(n<N) because the tail is now over n>=N
complement_N <- z:(N-1)
complement_P <- sum( (z/complement_N) * choose(complement_N,z) * theta^z * (1-theta)^(complement_N-z) )

lowTailP <- 1-complement_P

TwoTailP <- 2*lowTailP
TwoTailP
```
the two-tailed p value = 0.031.
0.031 < .05, so reject the null hypothesis theta=1/6
This conclusion difers from part A because we stopped at a different number of rolls in part B which resulted in a different sampling distribution. 



**Problem 3**

For the following examples identify the predicted variable and its scale type, identify the predictor variable(s) and its scale type, and identify what type of GLM this example belongs based on the table shown below (Figure 2 or Fig. 15.3 of the book).

a) [Hahn, Chater, and Richardson (2003)](https://pubmed.ncbi.nlm.nih.gov/12499105/) were interested in perceived similarity of simple geometric patterns. Human observers rated pairs of patterns for how similar the patterns appeared, by circling one of the digits 1–7 printed on the page, where 1 meant “very dissimilar” and 7 meant “very similar.” The authors presented a theory of perceived similarity, in which patterns are perceived to be dissimilar to the extent that it takes more geometric transformations to produce one pattern from the other. The theory specified the exact number of transformations needed to get from one pattern to the other.

Predicted Variable: number of transformations needed to get from one pattern to the other
- Scale type: metric 

Predictor Variable: human observer's ratings
- Scale type: metric  

GLM type: linear regression


b) [R. L. Berger, Boos, and Guess (1988)](https://pubmed.ncbi.nlm.nih.gov/3358985/) were interested in the longevity of rats, measured in days, as a function of the rat’s diet. One group of rats were fed freely, another group of rats had a very low calorie diet.

Predicted Variable:longevity of rats (in days)
 - Scale type: metric 
 
Predictor Variable: diet
- Scale type: nominal 

GLM type: logistic regression



